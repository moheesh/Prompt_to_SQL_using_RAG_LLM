"""
Streamlit App for SQL Learning Assistant
Integrates: RAG + Fine-tuned Model + Gemini Enhancement
"""

import streamlit as st
import os
import sys
from dotenv import load_dotenv

# Load environment variables FIRST
load_dotenv()

# Add parent directory
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# =============================================================================
# PAGE CONFIG - MUST BE FIRST STREAMLIT COMMAND
# =============================================================================

st.set_page_config(
    page_title="SQL Learning Assistant",
    page_icon="‚ö°",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =============================================================================
# CACHED LOADERS - Load on-demand, cache forever
# =============================================================================

@st.cache_resource(show_spinner=False)
def load_chromadb():
    """Download ChromaDB from HuggingFace if needed."""
    chromadb_path = "chromadb_data"
    hf_chromadb_id = os.getenv("HF_CHROMADB_ID", None)
    
    has_files = False
    if os.path.exists(chromadb_path):
        local_files = os.listdir(chromadb_path) if os.path.isdir(chromadb_path) else []
        has_files = any('chroma' in f.lower() or 'sqlite' in f.lower() for f in local_files) or len(local_files) > 2
    
    if not has_files and hf_chromadb_id:
        from huggingface_hub import snapshot_download
        os.makedirs(chromadb_path, exist_ok=True)
        snapshot_download(repo_id=hf_chromadb_id, repo_type="dataset", local_dir=chromadb_path)
    
    return chromadb_path

@st.cache_resource(show_spinner=False)
def load_retriever():
    """Load the RAG retriever."""
    load_chromadb()
    from rag.retriever import SQLRetriever
    return SQLRetriever()

@st.cache_resource(show_spinner=False)
def load_model():
    """Load the fine-tuned model."""
    from finetuning.inference import SQLGenerator
    return SQLGenerator()

@st.cache_resource(show_spinner=False)
def load_prompt_builder():
    """Load prompt builder."""
    from prompts.prompt_builder import PromptBuilder
    return PromptBuilder()

@st.cache_resource(show_spinner=False)
def load_gemini():
    """Load Gemini client."""
    from pipeline.integrated import GeminiClient, GEMINI_KEYS
    if GEMINI_KEYS:
        return GeminiClient()
    return None

# =============================================================================
# HELPER FUNCTION TO RUN PIPELINE
# =============================================================================

def run_pipeline(question, num_examples=3):
    """Run the full pipeline - loads components on first use."""
    result = {
        'question': question,
        'success': False,
        'steps': {}
    }
    
    # Step 1: RAG
    rag_context = ""
    examples = []
    try:
        with st.spinner("üîç Loading RAG system..."):
            retriever = load_retriever()
        if retriever:
            examples = retriever.retrieve(question, top_k=num_examples)
            rag_context = "Similar SQL examples:\n\n"
            for i, r in enumerate(examples, 1):
                rag_context += f"Example {i}:\nQuestion: {r['question']}\nSQL: {r['sql']}\n\n"
    except Exception as e:
        st.warning(f"RAG error: {e}")
    
    result['steps']['rag'] = {'examples': examples, 'num_examples': len(examples), 'context': rag_context}
    
    # Step 2: Prompt
    prompt = ""
    try:
        prompt_builder = load_prompt_builder()
        if prompt_builder:
            prompt_result = prompt_builder.build_prompt(question=question, rag_context=rag_context)
            if prompt_result['success']:
                prompt = prompt_result['prompt']
    except:
        pass
    if not prompt:
        prompt = f"{rag_context}\nQuestion: {question}\n\nSQL:"
    
    result['steps']['prompt'] = {'prompt': prompt, 'length': len(prompt)}
    
    # Step 3: Fine-tuned Model
    finetuned_sql = None
    try:
        with st.spinner("ü§ñ Loading AI model..."):
            model = load_model()
        if model:
            finetuned_sql = model.generate(question, rag_context)
    except Exception as e:
        st.warning(f"Model error: {e}")
    
    result['steps']['finetuned'] = {'sql': finetuned_sql, 'error': None if finetuned_sql else 'Model not available'}
    
    if not finetuned_sql:
        return result
    
    # Step 4: Gemini Enhancement
    enhanced_sql = finetuned_sql
    try:
        gemini = load_gemini()
        if gemini:
            enhance_prompt = f"""You are an SQL expert. Review and enhance this SQL query.

Original Question: {question}

Generated SQL (by a smaller model):
{finetuned_sql}

Rules:
- If the SQL is correct, return it unchanged
- If it needs fixes, return the corrected version
- Return ONLY the SQL query, no explanations

Enhanced SQL:"""
            response, error = gemini.generate(enhance_prompt)
            if response and not error:
                enhanced_sql = response.strip()
                if enhanced_sql.startswith("```"):
                    lines = enhanced_sql.split("\n")
                    enhanced_sql = "\n".join(lines[1:-1] if lines[-1] == "```" else lines[1:])
                if enhanced_sql.lower().startswith("sql"):
                    enhanced_sql = enhanced_sql[3:].strip()
    except Exception as e:
        st.warning(f"Gemini enhance error: {e}")
    
    result['steps']['gemini_enhance'] = {'sql': enhanced_sql, 'info': {'enhanced': enhanced_sql != finetuned_sql}}
    result['final_sql'] = enhanced_sql
    
    # Step 5: Explanation
    explanation = ""
    try:
        gemini = load_gemini()
        if gemini:
            explain_prompt = f"Explain this SQL query in simple terms (2-3 sentences):\n\nSQL: {enhanced_sql}"
            response, error = gemini.generate(explain_prompt)
            if response and not error:
                explanation = response.strip()
    except:
        pass
    
    result['explanation'] = explanation
    result['success'] = True
    
    return result

# =============================================================================
# CUSTOM CSS
# =============================================================================

st.markdown("""
<style>
    .stApp {
        background: linear-gradient(135deg, #0f0f23 0%, #1a1a2e 50%, #16213e 100%);
    }
    
    .main-header {
        font-size: 3rem;
        font-weight: 800;
        background: linear-gradient(120deg, #00d4ff, #7c3aed, #f472b6);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    
    .sub-header {
        font-size: 1.1rem;
        color: #94a3b8;
        text-align: center;
        margin-bottom: 2rem;
    }
    
    .stButton > button {
        background: linear-gradient(135deg, #1e293b 0%, #334155 100%);
        color: #e2e8f0;
        border: 1px solid #475569;
        border-radius: 10px;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        background: linear-gradient(135deg, #3b82f6 0%, #8b5cf6 100%);
        border-color: #60a5fa;
        transform: translateY(-2px);
    }
    
    .stTextInput > div > div > input {
        background: rgba(30, 41, 59, 0.8);
        border: 1px solid #475569;
        border-radius: 12px;
        color: #f1f5f9;
    }
    
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, #0f172a 0%, #1e293b 100%);
    }
    
    .pipeline-box {
        background: rgba(30, 41, 59, 0.6);
        border: 1px solid #475569;
        border-radius: 8px;
        padding: 0.5rem 1rem;
        margin: 0.25rem 0;
        font-size: 0.85rem;
        text-align: center;
    }
    
    .pipeline-arrow {
        color: #3b82f6;
        text-align: center;
        font-size: 1.2rem;
    }
</style>
""", unsafe_allow_html=True)

# =============================================================================
# HEADER
# =============================================================================

st.markdown('<p class="main-header">‚ö° SQL Learning Assistant</p>', unsafe_allow_html=True)
st.markdown('<p class="sub-header">Transform Natural Language into SQL using AI-Powered Pipeline</p>', unsafe_allow_html=True)

# =============================================================================
# SIDEBAR
# =============================================================================

with st.sidebar:
    st.markdown("## ‚öôÔ∏è Configuration")
    st.markdown("---")
    
    st.markdown("### üéØ RAG Settings")
    num_examples = st.slider("Similar examples to retrieve", min_value=1, max_value=5, value=3)
    
    st.markdown("---")
    
    st.markdown("### üìä System Status")
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("‚úÖ **RAG**")
        st.markdown("‚úÖ **Model**")
    with col2:
        st.markdown("‚úÖ **Prompts**")
        if os.getenv("GEMINI_API_KEY"):
            st.markdown("‚úÖ **Gemini**")
        else:
            st.markdown("‚ùå **Gemini**")
    
    st.markdown("---")
    
    st.markdown("### üîÑ Pipeline Flow")
    pipeline_steps = [
        ("üì¶", "Synthetic Data"),
        ("üéì", "Fine-tuned Model"),
        ("‚ùì", "User Question"),
        ("üîç", "RAG Retrieval"),
        ("üìù", "Prompt Engineering"),
        ("ü§ñ", "Model Inference"),
        ("‚ú®", "Gemini Enhancement"),
        ("‚úÖ", "Final Output"),
    ]
    
    for i, (icon, title) in enumerate(pipeline_steps):
        st.markdown(f'<div class="pipeline-box">{icon} <strong>{title}</strong></div>', unsafe_allow_html=True)
        if i < len(pipeline_steps) - 1:
            st.markdown('<p class="pipeline-arrow">‚Üì</p>', unsafe_allow_html=True)
    
    st.markdown("---")
    st.markdown("### üìö About")
    st.markdown("**Course:** INFO7375")

# =============================================================================
# MAIN CONTENT
# =============================================================================

if "messages" not in st.session_state:
    st.session_state.messages = []

if "results_history" not in st.session_state:
    st.session_state.results_history = []

if "input_text" not in st.session_state:
    st.session_state.input_text = ""

# =============================================================================
# EXAMPLE QUESTIONS
# =============================================================================

st.markdown("### üí° Try an Example")

example_questions = [
    ("üë• Employees", "Find all employees with salary above 50000"),
    ("üìä Orders", "Count total orders by customer"),
    ("üèÜ Top Products", "Show top 5 products by revenue"),
    ("üìÖ Recent", "List customers who placed orders in 2024"),
    ("üí∞ Salary", "Calculate average salary by department"),
]

cols = st.columns(5)
for i, (label, ex_question) in enumerate(example_questions):
    with cols[i]:
        if st.button(label, key=f"ex_{i}", use_container_width=True, help=ex_question):
            st.session_state.input_text = ex_question

# =============================================================================
# INPUT AREA
# =============================================================================

st.markdown("### üé§ Ask Your Question")

col1, col2 = st.columns([6, 1])

with col1:
    question = st.text_input(
        "Question",
        placeholder="e.g., Find all employees with salary greater than 50000...",
        label_visibility="collapsed",
        key="input_text"
    )

with col2:
    submit_btn = st.button("üöÄ Run", type="primary", use_container_width=True)

st.markdown("---")

# =============================================================================
# CHAT HISTORY
# =============================================================================

for i, message in enumerate(st.session_state.messages):
    with st.chat_message(message["role"], avatar="üßë‚Äçüíª" if message["role"] == "user" else "ü§ñ"):
        st.markdown(message["content"])
        
        if message["role"] == "assistant":
            result_idx = i // 2
            if result_idx < len(st.session_state.results_history):
                result = st.session_state.results_history[result_idx]
                if result and result.get('success'):
                    with st.expander("üîç View Pipeline Details", expanded=False):
                        tab1, tab2, tab3, tab4 = st.tabs(["üîç RAG", "üìù Prompt", "ü§ñ Fine-tuned", "‚ú® Gemini"])
                        
                        with tab1:
                            examples = result['steps']['rag'].get('examples', [])
                            st.markdown(f"**Retrieved {len(examples)} examples**")
                            for j, ex in enumerate(examples, 1):
                                st.markdown(f"**Example {j}** | Score: `{ex.get('score', 0):.3f}`")
                                st.markdown(f"Q: {ex.get('question', 'N/A')}")
                                st.code(ex.get('sql', 'N/A'), language="sql")
                        
                        with tab2:
                            st.markdown("**Constructed Prompt:**")
                            st.code(result['steps']['prompt'].get('prompt', 'N/A'), language="text")
                        
                        with tab3:
                            st.markdown("**Fine-tuned Model Output:**")
                            st.code(result['steps']['finetuned'].get('sql', 'N/A'), language="sql")
                        
                        with tab4:
                            if 'gemini_enhance' in result['steps']:
                                st.markdown("**Enhanced SQL:**")
                                st.code(result['steps']['gemini_enhance'].get('sql', 'N/A'), language="sql")

# =============================================================================
# PROCESS QUERY
# =============================================================================

if submit_btn and question:
    st.session_state.messages.append({"role": "user", "content": question})
    
    with st.chat_message("user", avatar="üßë‚Äçüíª"):
        st.markdown(question)
    
    with st.chat_message("assistant", avatar="ü§ñ"):
        with st.status("üîÑ Processing your query...", expanded=True) as status:
            st.write("üîç Retrieving similar examples...")
            st.write("üìù Building prompt...")
            st.write("ü§ñ Generating SQL...")
            st.write("‚ú® Enhancing with Gemini...")
            
            result = run_pipeline(question=question, num_examples=num_examples)
            
            status.update(label="‚úÖ Complete!", state="complete", expanded=False)
        
        st.session_state.results_history.append(result)
        
        if result['success']:
            st.markdown("### ‚úÖ Generated SQL")
            st.code(result['final_sql'], language="sql")
            
            if 'gemini_enhance' in result['steps']:
                original = result['steps']['finetuned'].get('sql', '')
                enhanced = result['steps']['gemini_enhance'].get('sql', '')
                if original != enhanced:
                    st.success("‚ú® Query optimized by Gemini!")
                else:
                    st.info("‚úì Query was already optimal")
            
            if 'explanation' in result and result['explanation']:
                if not result['explanation'].startswith("Explanation error"):
                    st.markdown("### üìñ Explanation")
                    st.info(result['explanation'])
            
            with st.expander("üîç View Pipeline Details", expanded=False):
                tab1, tab2, tab3, tab4 = st.tabs(["üîç RAG", "üìù Prompt", "ü§ñ Fine-tuned", "‚ú® Gemini"])
                
                with tab1:
                    examples = result['steps']['rag'].get('examples', [])
                    st.markdown(f"**Retrieved {len(examples)} examples**")
                    for j, ex in enumerate(examples, 1):
                        st.markdown(f"**Example {j}** | Score: `{ex.get('score', 0):.3f}`")
                        st.markdown(f"Q: {ex.get('question', 'N/A')}")
                        st.code(ex.get('sql', 'N/A'), language="sql")
                
                with tab2:
                    st.markdown("**Constructed Prompt:**")
                    st.code(result['steps']['prompt'].get('prompt', 'N/A'), language="text")
                
                with tab3:
                    st.markdown("**Fine-tuned Model Output:**")
                    st.code(result['steps']['finetuned'].get('sql', 'N/A'), language="sql")
                
                with tab4:
                    if 'gemini_enhance' in result['steps']:
                        st.markdown("**Enhanced SQL:**")
                        st.code(result['steps']['gemini_enhance'].get('sql', 'N/A'), language="sql")
            
            response_text = f"**Generated SQL:**\n```sql\n{result['final_sql']}\n```"
            if 'explanation' in result and not result['explanation'].startswith("Explanation error"):
                response_text += f"\n\n**Explanation:** {result['explanation']}"
            
            st.session_state.messages.append({"role": "assistant", "content": response_text})
        
        else:
            st.error("‚ùå Failed to generate SQL. Please try again.")
            st.session_state.messages.append({"role": "assistant", "content": "‚ùå Failed to generate SQL."})

elif submit_btn and not question:
    st.warning("‚ö†Ô∏è Please enter a question first!")

# =============================================================================
# FOOTER
# =============================================================================

st.markdown("---")

col1, col2, col3 = st.columns([1, 2, 1])

with col1:
    if st.button("üóëÔ∏è Clear Chat", use_container_width=True):
        st.session_state.messages = []
        st.session_state.results_history = []
        st.session_state.input_text = ""
        st.rerun()

with col2:
    st.markdown('<p style="text-align: center; color: #64748b;">Built with ‚ù§Ô∏è using Streamlit ‚Ä¢ LangChain ‚Ä¢ Gemini</p>', unsafe_allow_html=True)

with col3:
    st.markdown('<p style="text-align: right; color: #64748b;"><strong>INFO7375</strong></p>', unsafe_allow_html=True)